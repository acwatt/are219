

\textbf{Comment 1.} The economic story behind an empirical observation is most often (though not always) a story of incentives. You describe data anomalies, but without setting the context in terms of incentives. Critically, we would like to know more if possible about who has the discretion to decide what data to omit, and what their incentives are. The introduction of the paper presumes a familiarity with local actor incentives to stay in attainment, but this may be assuming too much for a general economic audience, as opposed to someone specializing in air pollution. So, this comment pertains both to explaining the importance of attainment status and who the key stakeholders are to the broader audience, and offering additional insight on the details of who the “data managers” are, who hires them, and whether they are plausibly subject to pressure.

\noindent\textbf{Response to Comment 1.} 

\noindent\textbf{Comment 2.} Another exercise would be to ask how much room is left for additional manipulation. That is, suppose a data controller cared about boosting numbers and wanted to drop as many data points as would be allowed. Given the data that are being reported and the slack in the number dropped, how much of a difference would additional manipulation make? This is sort of a scale exercise for asking how useful manipulation might be.

\noindent\textbf{Response to Comment 2.}

\noindent\textbf{Comment 3.} A related big picture comment is to ask why the objective is to infer an estimate for the missing data, rather than simply look for evidence of manipulation. (To be sure, they are related, but we think they are not identical.) You might have begun, for example, by predicting whether an EPA data point was missing (binary outcome), and asking whether high purple air readings make it more likely an observation is missing.

More generally, one might argue that this paper is a very interesting measurement exercise in search of an economic application. The possibility of flipping attainment status is one way to turn this into an economics/policy narrative, but you might also consider something more broad, like stating that “effective regulation requires reliable measurement,” and then asking if the measure here is manipulable. Obviously we know that there are other types of manipulation (from prior literature), so your contribution is to look for the possibility of manipulation in this particular mechanism.

\noindent\textbf{Comment 4.}  Isn't a kernel regression the right approach here?  Asymptotic performance of current approach (inverse distance) may not work?
   
\noindent\textbf{Comment 5.} First table seems to involve regression to the mean (Purple Air a rhs variable, with measurement error).
 
\noindent\textbf{Comment 6.} Multiple testing (however many sites...)?  If drawing inference that a *particular* site is engaged in manipulation, current version of inference seems problematic.

\noindent\textbf{Comment  7.} Confidence intervals for non-compliant site highly asymmetric---not clear why?

\noindent\textbf{Response to Comment 7.} A brief explanation of the asymmetry of the confidence intervals has been added to section \ref{results} and a detailed discussion has been added to appendix section \ref{sec:design_values}.
